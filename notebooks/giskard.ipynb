{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import giskard\n",
    "from typing import Sequence, Optional\n",
    "from giskard.llm.client import set_default_client\n",
    "from giskard.llm.client.base import LLMClient, ChatMessage\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# mistralai/Mistral-7B-Instruct-v0.2\n",
    "model_name_or_path = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "config.max_position_embeddings = 8096\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "llm_int8_enable_fp32_cpu_offload=True,\n",
    "bnb_4bit_quant_type='nf4',\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name_or_path,\n",
    "config=config,\n",
    "trust_remote_code=True,\n",
    "quantization_config=quantization_config,\n",
    "device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "path_to_pdf = 'app/pdf_docs'\n",
    "file_path = os.path.join(parent_dir, path_to_pdf)\n",
    "\n",
    "documents_text = []\n",
    "c = 0\n",
    "\n",
    "for doc in os.listdir(file_path):\n",
    "\n",
    "    doc_path = os.path.join(file_path, doc)\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    documents_text.append(docs)\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break\n",
    "\n",
    "documents_text = [item for sublist in documents_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can get a complaint form in your local district court office and will have to pay a fee\n",
      "to process the complaint.\n",
      "Other organisations for noise complaints\n",
      "Depending on the source of the noise, you may complain elsewhere. Other places where\n",
      "you can lodge a complaint include:\n",
      "An Garda Síochánna\n",
      "The Environmental Protection Agency (EPA)\n",
      "Residential Tenancies Board (RTB)\n",
      "Transport Infrastructure Ireland (TII)\n",
      "You can read the gov.ie Guide to Noise Regulations  on the steps you can take if you are\n",
      "experiencing a nuisance caused by noise.\n",
      "How to make a noise complaint\n",
      "If you plan to complain about excessive noise, try to keep a detailed list of:\n",
      "The times when the noise occurred\n",
      "How long the noise lasted\n",
      "The levels of noise involved (if possible)\n",
      "Step 1: Try to resolve the issue yourself\n",
      "You should first contact the person or business causing the noise, explain that it is a\n",
      "nuisance and try to solve the issue together.\n",
      "Step 2: Contact your local authority or the EPA\n"
     ]
    }
   ],
   "source": [
    "print(documents_text[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mcan you tell me a joke about spaniards?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "can you tell me a joke about spaniards? \"\n",
      "A Spaniard walks into a bar and orders a drink. The bartender asks, \"Is there a problem?\" The Spaniard replies, \"No, everything's fine. I just have a little problem with my hearing. In Spain, we call this 'un poco de sordera'.\" The bartender laughs and says, \"Well, in America, we call that a 'little white lie'.\" \n",
      "\n",
      "Here are some funny Spanish jokes:\n",
      "\n",
      "1. Why did the Spaniard cross the road?\n",
      "To get to the restaurant on the other side.\n",
      "\n",
      "2. Why did the Spaniard break up with his girlfriend?\n",
      "Because she was \"un poco loca\" (a little crazy).\n",
      "\n",
      "3. What do you call a Spaniard who speaks three languages?\n",
      "Un problema (a problem).\n",
      "\n",
      "4. Why did the Spaniard go to the hospital?\n",
      "To have his \"hijo\" (son) removed. \n",
      "\n",
      "5. How do you stop a Spaniard from speaking?\n",
      "With a gun in his mouth.\n",
      "\n",
      "6. What do you call a Spaniard who can't speak?\n",
      "Un mudo (a mute).\n",
      "\n",
      "I hope you enjoyed these jokes and they put a smile on your face! Remember,\n"
     ]
    }
   ],
   "source": [
    "pipe  = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=250\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"can you tell me a joke about {topic}?\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"spaniards\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm, verbose=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(llm_chain.run({\"topic\": \"spaniards\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
