{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import giskard\n",
    "from typing import Sequence, Optional\n",
    "from giskard.llm.client import set_default_client\n",
    "from giskard.llm.client.base import LLMClient, ChatMessage\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  7.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# mistralai/Mistral-7B-Instruct-v0.2\n",
    "model_name_or_path = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "config.max_position_embeddings = 8096\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "llm_int8_enable_fp32_cpu_offload=True,\n",
    "bnb_4bit_quant_type='nf4',\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name_or_path,\n",
    "config=config,\n",
    "trust_remote_code=True,\n",
    "quantization_config=quantization_config,\n",
    "device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mcan you tell me a joke about spaniards?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "can you tell me a joke about spaniards? \"\n",
      "A Spaniard walks into a bar and orders a drink. The bartender asks, \"Is there a problem?\" The Spaniard replies, \"No, everything's fine. I just have a little problem with my hearing. In Spain, we call this 'un poco de sordera'.\" The bartender laughs and says, \"Well, in America, we call that a 'little white lie'.\" \n",
      "\n",
      "Here are some funny Spanish jokes:\n",
      "\n",
      "1. Why did the Spaniard cross the road?\n",
      "To get to the restaurant on the other side.\n",
      "\n",
      "2. Why did the Spaniard break up with his girlfriend?\n",
      "Because she was \"un poco loca\" (a little crazy).\n",
      "\n",
      "3. What do you call a Spaniard who speaks three languages?\n",
      "Un problema (a problem).\n",
      "\n",
      "4. Why did the Spaniard go to the hospital?\n",
      "To have his \"hijo\" (son) removed. \n",
      "\n",
      "5. How do you stop a Spaniard from speaking?\n",
      "With a gun in his mouth.\n",
      "\n",
      "6. What do you call a Spaniard who can't speak?\n",
      "Un mudo (a mute).\n",
      "\n",
      "I hope you enjoyed these jokes and they put a smile on your face! Remember,\n"
     ]
    }
   ],
   "source": [
    "pipe  = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=250\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"can you tell me a joke about {topic}?\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"spaniards\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm, verbose=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(llm_chain.run({\"topic\": \"spaniards\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Projects/citizens-info/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Projects/citizens-info/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app/pdf_docs/_en_environment_pollution_noise-regulations_.pdf'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "path_to_pdf = 'app/pdf_docs'\n",
    "file_path = os.path.join(parent_dir, path_to_pdf)\n",
    "\n",
    "documents_text = []\n",
    "c = 0\n",
    "\n",
    "for doc in os.listdir(file_path):\n",
    "\n",
    "    doc_path = os.path.join(file_path, doc)\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    documents_text.append(docs)\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break\n",
    "\n",
    "documents_text = [item for sublist in documents_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accept\\tall\\tcookies\\nManage\\tmy\\tpreferences\\nReject\\tcookies'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_text[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept\tall\tcookies\n",
      "Manage\tmy\tpreferences\n",
      "Reject\tcookies\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "with open('/root/Projects/citizens-info/app/pdf_docs/_en_birth-family-relationships_adoption-and-fostering_surrogacy_.pdf', 'rb') as f:\n",
    "    pdf = PyPDF2.PdfReader(f)\n",
    "    text = ''\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text ()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got multiple values for argument 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m options \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChromeOptions()\n\u001b[1;32m      4\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadless\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mchromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: WebDriver.__init__() got multiple values for argument 'options'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('C:\\\\chromedriver', options=options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
