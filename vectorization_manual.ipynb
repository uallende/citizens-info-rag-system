{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    "    additional_config=weaviate.config.AdditionalConfig(timeout=(60, 180)),\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": openai_api_key  # Replace with your inference API key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdf = 'pdf_docs'\n",
    "\n",
    "documents_text = []\n",
    "\n",
    "for doc in os.listdir(path_to_pdf):\n",
    "\n",
    "    doc_path = f'{path_to_pdf}/{doc}'\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    documents_text.append(docs)\n",
    "\n",
    "documents_text = [item for sublist in documents_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/m_enmb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModel\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "encoder = AutoModel.from_pretrained(\n",
    "    'Salesforce/SFR-Embedding-Mistral',\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral')\n",
    "input_texts = 'My favorite condiment is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    \n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096\n",
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "outputs = encoder(**batch_dict)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
